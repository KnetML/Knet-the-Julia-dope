{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest form of a fully recurrent neural network is a multi layer perceptron (MLP) with the previous set of *hidden unit activations* feeding back into the network along with the inputs. First, recall that a simple MLP has $x$ and $y$ as input and output vectors with nonlinear activation layers between them, except perhaps on the readout (or output) layer:\n",
    "\n",
    "<img src=\"../images/MLP_classic.jpg\" alt=\"Drawing\" style=\"width: 50px;\"/>\n",
    "\n",
    "Formally, for an MLP with $L$ layers, the output of layer $l$ is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "h^l= f_H\\big(W^lh^{l-1} + b^l\\big)\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\hspace{0.05cm} y = f_O\\big(W^Oh^{L} + b^O\\big)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "where $h^0=x$ is the input and $h^l$ is the activation unit of layer $l$. \n",
    "\n",
    "For example, consider the single-layer MLP implementation below. Notice that the parameters of the weight matrices are separated into four pars: two for the single hidden layer ($W^1,b^1$) and two for the output layer ($W^O,b^O$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function mlp1(param, x)\n",
    "    h = tanh(x * param[1] .+ param[2]) \n",
    "    y = h * param[3] .+ param[4]\n",
    "    return y\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast, a single-layer RNN becomes time-dependent, taking the previous hidden state $h_{t-1}$ as an extra input and returning the next hidden state $h_t$ as an extra output:\n",
    "\n",
    "<img src=\"../images/MLP_rnn.jpg\" alt=\"Drawing\" style=\"width: 100px;\"/>\n",
    "\n",
    "To see how this done, first we make $h^l$ and $y$ time-dependent (recalling that $h^0=x$):\n",
    "\n",
    "\\begin{equation}\n",
    "h^l_t= f_H\\big(W^lh^{l-1}_t + b^l\\big)\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\hspace{0.05cm} y_t = f_O\\big(W^Oh^{L}_t + b^O\\big)\n",
    "\\end{equation}\n",
    "\n",
    "So far we haven't done anything: all weight matrices are still time-independent and we are simply focusing on the propagation of a single sample pair at time $t$. An RNN layer also takes the hidden state at a previous time *from the same layer* as an input, i.e. in addition to $h^{l-1}_t$, layer $l$ will also take $h^l_{t-1}$ as an input:\n",
    "\n",
    "\\begin{equation}\n",
    "h^l_t= f_H\\big(W^l_1h^{l-1}_t + W^l_2h^l_{t-1} + b^l\\big)\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\hspace{0.05cm} y_t = f_O\\big(W^Oh^{L}_t + b^O\\big)\n",
    "\\end{equation}\n",
    "\n",
    "Notice that the output layer is unchanged. Let us consider a single-layer RNN: For simplicity, note that we can combine the weight matrices $W_1$ and $W_2$ into a single matrix by concatenating both inputs. Further, we drop the superscript $l$ for clarity: \n",
    "\n",
    "\\begin{equation}\n",
    "h_t= f_H\\big(W\\,[x_t\\,\\,h_{t-1}] + b)\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\hspace{0.05cm} y_t = f_O\\big(W^Oh_t + b^O\\big)\n",
    "\\end{equation}\n",
    "\n",
    "This simple single-layer RNN is implemented below with $h_{t-1}\\equiv h_{t^-}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mlp1_rnn (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function mlp1_rnn(param, hₜ₋, xₜ)\n",
    "    input  = hcat(hₜ₋, xₜ)\n",
    "    hₜ = tanh(input * param[1] .+ param[2])\n",
    "    yₜ = hₜ * param[3] .+ param[4]\n",
    "    return (hₜ, xₜ)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> Advanced Discussion</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size of Weight Matrices\n",
    "\n",
    "We've seen that a single-layer rnn has the following form:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "h_t &= f_H\\big(W_1x_t + W_2h_{t-1} + b\\big)\\\\\n",
    "y_t &= f_O\\big(W^Oh_t + b^O\\big)\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Let $I=length(x_t)$ be the size of the input, and $H=length(h_t)$ be the size of the hidden output unit. Then, the size of the weight matrices are $IH$ for $W_1$ and $HH$ for $W_2$. With $O=length(y_t)$, we may re-write these equations to include the size of the weight matrices as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "h_t &= f_H\\big(W_{IH}x_t + W_{HH}h_{t-1} + b_H\\big)\\\\\n",
    "y_t &= f_O\\big(W_{HO}h_t + b_O\\big)\n",
    "\\end{aligned}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-layer MLP RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a single-layer RNN it was easy to understand the implementation since $h^{l-1}_t\\equiv x_t$ with $l=1$. To illustrate how this works for multi-layer RNNs, first note that the output of an activation function $h^l_t$ is in essence passed forward through the RNN at time $t$, but delayed within its own layer to be passed at a later time $t+1$:\n",
    "\n",
    "<img src=\"../images/MLP_rnn_full.jpg\" alt=\"Drawing\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In addition, while so far we have assumed that each RNN layer has identical form, i.e. they are all represented by the same structure $A$, in general each layer may have different structure $A_l$. For example, a generic RNN could be created from a combination of the following layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mlp_rnn_relu (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function mlp_rnn_tanh(param, hₜ₋, xₜ)\n",
    "    input  = hcat(hₜ₋, xₜ)\n",
    "    hₜ = tanh(input * param[1] .+ param[2])\n",
    "    return (hₜ, xₜ)\n",
    "end\n",
    "\n",
    "function mlp_rnn_relu(param, hₜ₋, xₜ)\n",
    "    input  = hcat(hₜ₋, xₜ)\n",
    "    hₜ = max.(0, input * param[1] .+ param[2])\n",
    "    return (hₜ, xₜ)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.0-pre.alpha",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
