{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Julia Things"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment\n",
    "\n",
    "First things first. Let us set up the environment with the requried packages for this notebook. We will also set the desired context (e.g. `KnetArray` for gpu), the number of epochs (`nepochs`), and the variable `fast`. This variable is used to skip checking the accuracy at every epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OS: Linux\n",
      "Julia: 0.6.0\n",
      "Knet: 0.8.5+\n",
      "GPU: NVS 310\n",
      "TITAN X (Pascal)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for p in (\"Knet\", \"Plots\")\n",
    "    Pkg.installed(p) == nothing && Pkg.add(p)\n",
    "end\n",
    "\n",
    "using Knet, Plots\n",
    "gr()\n",
    "\n",
    "Knet.gpu(0); # set the desired GPU to use\n",
    "atype   = KnetArray; # atype = KnetArray{Float32} for gpu usage, Array{Float32} for cpu. \n",
    "nepochs = 10\n",
    "fast    = false\n",
    "\n",
    "println(\"OS: \", Sys.KERNEL)\n",
    "println(\"Julia: \", VERSION)\n",
    "println(\"Knet: \", Pkg.installed(\"Knet\"))\n",
    "println(\"GPU: \", readstring(`nvidia-smi --query-gpu=name --format=csv,noheader`))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Stuff\n",
    "\n",
    "In this notebook we introduce the following Julia/Knet packages and functions:\n",
    "\n",
    "* Knet's function [conv4](http://denizyuret.github.io/Knet.jl/latest/reference.html#Knet.conv4): Execute convolutions or cross-correlations using filters specified with `w` over tensor `x`.\n",
    "* Knet's function [pool](http://denizyuret.github.io/Knet.jl/latest/reference.html#Knet.pool): Compute pooling of input values (i.e., the maximum or average of several adjacent values) to produce an output with smaller height and/or width."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional neural networks (CNNs)\n",
    "\n",
    "In the [previous example](../chapter03_deep-neural-networks/section2-multilayer perceptrons.ipynb), we connected the nodes of our neural networks in what seems like the simplest possible way. Every node in each layer was connected to every node in the subsequent layers. \n",
    "\n",
    "![](https://github.com/zackchase/mxnet-the-straight-dope/blob/master/img/multilayer-perceptron.png?raw=true)\n",
    "\n",
    "This can require a lot of parameters! If our input were a 256x256 color image (still quite small for a photograph), and our network had 1,000 nodes in the first hidden layer, then our first weight matrix would require (256x256x3)x1000 parameters. That's nearly 200 million. Moreover the hidden layer would ignore all the spatial structure in the input image even though we know the local structure represents a powerful source of prior knowledge. \n",
    "\n",
    "Convolutional neural networks incorporate convolutional layers. These layers associate each of their nodes with a small window, called a *receptive field*, in the previous layer, instead of connecting to the full layer. This allows us to first learn local features via transformations that are applied in the same way for the top right corner as for the bottom left. Then we collect all this local information to predict global qualities of the image (like whether or not it depicts a dog). \n",
    "\n",
    "![](http://cs231n.github.io/assets/cnn/depthcol.jpeg)\n",
    "(Image credit: Stanford cs231n http://cs231n.github.io/assets/cnn/depthcol.jpeg)\n",
    "\n",
    "In short, there are two new concepts you need to grok here. First, we'll be introducting *convolutional* layers. Second, we'll be interleaving them with *pooling* layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mLoading MNIST...\n",
      "\u001b[39m"
     ]
    }
   ],
   "source": [
    "include(Knet.dir(\"data\",\"mnist.jl\"))\n",
    "xtrn, ytrn, xtst, ytst = mnist()\n",
    "num_samples = 1000;\n",
    "batch_size  = 10;\n",
    "dtrn = minibatch(xtrn[:, :, :, 1:num_samples], ytrn[1:num_samples], batch_size, xtype=atype);\n",
    "dtst = minibatch(xtst[:, :, :, 1:num_samples], ytst[1:num_samples], batch_size, xtype=atype);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Parameters\n",
    "\n",
    "Each node in convolutional layer is associated with a 3D block (`height` x `width` x `channel`) in the input tensor. Moreover, the convolutional layer itself has multiple output channels. So the layer is parameterized by a 4 dimensional weight tensor, commonly called a *convolutional kernel*.\n",
    "\n",
    "The output is produced by sliding the kernel across the input image skipping locations according to a pre-defined *stride* (but we'll just assume that to be 1 in this tutorial). Let's initialize some such kernels from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (kx, ky, input_size, output_size)\n",
    "initweights(atype) = map(atype, Any[ xavier(Float32,5,5,1,20),  zeros(Float32,1,1,20,1),\n",
    "                                     xavier(Float32,5,5,20,50), zeros(Float32,1,1,50,1),\n",
    "                                     xavier(Float32,500,800),   zeros(Float32,500,1),\n",
    "                                     xavier(Float32,10,500),    zeros(Float32,10,1) ]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of each kernel is ($k_x, k_y$, ch_input, ch_output), where $k_x, k_y$ are the kernel size (sliding window size), ch_input is the number of input channels (1 for gray scale, 3 for RGB), and ch_output is the number of output channels (or output filters). Basically, the number of output channels is really the number of filters we want to create. Each new filter is convolved with each input filter. For the first layer the input is the image $x$, therefore we convolve 20 filters with a single input image (for RGB, we convolved all three images). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model \n",
    "\n",
    "We will build our `predict` function by introducing two new functions. First, we will new Knet's function [conv4](http://denizyuret.github.io/Knet.jl/latest/reference.html#Knet.conv4) to execute convolutions or cross-correlations using filters specified with `w` over tensor `x`. After that we will use Knet's function [pool](http://denizyuret.github.io/Knet.jl/latest/reference.html#Knet.pool). Pooling gives us a way to downsample in the spatial dimensions. Early convnets typically used average pooling, but max pooling tends to give better results. \n",
    "\n",
    "![](https://qph.ec.quoracdn.net/main-qimg-8afedfb2f82f279781bfefa269bc6a90)\n",
    "\n",
    "A typical pooling layer is defined by the `window size` (typically 2x2), `stride size` (typically (2,2)), and `pooling type`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function predict(w,x0)\n",
    "    x1 = pool(relu.(conv4(w[1],x0) .+ w[2]))\n",
    "    x2 = pool(relu.(conv4(w[3],x1) .+ w[4]))\n",
    "    x3 = relu.(w[5]*mat(x2) .+ w[6])\n",
    "    return w[7]*x3 .+ w[8]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss(w, x, ygold, predict) = nll(predict(w, x), ygold);\n",
    "lossgradient = grad(loss);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train(w, dtrn, optim, predict; epochs=10)\n",
    "    \n",
    "    for epoch = 1:epochs\n",
    "        for (x, y) in dtrn\n",
    "            g = lossgradient(w, x, y, predict)\n",
    "            update!(w, g, optim)\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optim(w; lr=0.1) = optimizers(w, Sgd;  lr=lr);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "report (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function report(epoch, w, dtrn, dtst, predict)\n",
    "    println((:epoch, epoch, :trn, accuracy(w, dtrn, predict), :tst, accuracy(w, dtst, predict)))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "\u001b[91mcudnn.cudnnConvolutionBackwardFilter error 3\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mcudnn.cudnnConvolutionBackwardFilter error 3\u001b[39m",
      "",
      "Stacktrace:",
      " [1] \u001b[1mmacro expansion\u001b[22m\u001b[22m at \u001b[1m/home/manuel/.julia/v0.6/Knet/src/gpu.jl:13\u001b[22m\u001b[22m [inlined]",
      " [2] \u001b[1m#conv4w#196\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Ptr{Void}, ::Int64, ::Array{Any,1}, ::Function, ::Knet.KnetArray{Float32,4}, ::Knet.KnetArray{Float32,4}, ::Knet.KnetArray{Float32,4}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/home/manuel/.julia/v0.6/Knet/src/conv.jl:70\u001b[22m\u001b[22m",
      " [3] \u001b[1m#conv4w#228\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Any,1}, ::Function, ::AutoGrad.Rec{Knet.KnetArray{Float32,4}}, ::Knet.KnetArray{Float32,4}, ::Knet.KnetArray{Float32,4}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./<missing>:0\u001b[22m\u001b[22m",
      " [4] \u001b[1m(::Knet.##conv4#204#213)\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Any,1}, ::Function, ::Type{AutoGrad.Grad{1}}, ::Knet.KnetArray{Float32,4}, ::Knet.KnetArray{Float32,4}, ::AutoGrad.Rec{Knet.KnetArray{Float32,4}}, ::Knet.KnetArray{Float32,4}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./<missing>:0\u001b[22m\u001b[22m",
      " [5] \u001b[1mconv4\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Type{AutoGrad.Grad{1}}, ::Knet.KnetArray{Float32,4}, ::Knet.KnetArray{Float32,4}, ::AutoGrad.Rec{Knet.KnetArray{Float32,4}}, ::Knet.KnetArray{Float32,4}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./<missing>:0\u001b[22m\u001b[22m",
      " [6] \u001b[1mbackward_pass\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::AutoGrad.Rec{Array{Knet.KnetArray{Float32,N} where N,1}}, ::AutoGrad.Rec{Float32}, ::Array{AutoGrad.Node,1}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/home/manuel/.julia/v0.6/AutoGrad/src/core.jl:254\u001b[22m\u001b[22m",
      " [7] \u001b[1m(::AutoGrad.##gradfun#1#3{#loss,Int64})\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Any,1}, ::Function, ::Array{Knet.KnetArray{Float32,N} where N,1}, ::Vararg{Any,N} where N\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/home/manuel/.julia/v0.6/AutoGrad/src/core.jl:40\u001b[22m\u001b[22m",
      " [8] \u001b[1m(::AutoGrad.#gradfun#2)\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Knet.KnetArray{Float32,N} where N,1}, ::Vararg{Any,N} where N\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/home/manuel/.julia/v0.6/AutoGrad/src/core.jl:39\u001b[22m\u001b[22m",
      " [9] \u001b[1m#train#11\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Int64, ::Function, ::Array{Knet.KnetArray{Float32,N} where N,1}, ::Knet.MB, ::Array{Knet.Sgd,1}, ::Function\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./In[6]:5\u001b[22m\u001b[22m",
      " [10] \u001b[1m(::#kw##train)\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Any,1}, ::#train, ::Array{Knet.KnetArray{Float32,N} where N,1}, ::Knet.MB, ::Array{Knet.Sgd,1}, ::Function\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./<missing>:0\u001b[22m\u001b[22m",
      " [11] \u001b[1mmacro expansion\u001b[22m\u001b[22m at \u001b[1m./In[11]:8\u001b[22m\u001b[22m [inlined]",
      " [12] \u001b[1manonymous\u001b[22m\u001b[22m at \u001b[1m./<missing>:?\u001b[22m\u001b[22m",
      " [13] \u001b[1minclude_string\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::String, ::String\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./loading.jl:515\u001b[22m\u001b[22m"
     ]
    }
   ],
   "source": [
    "w   = initweights(atype);\n",
    "opt = optim(w);\n",
    "\n",
    "if fast\n",
    "    train(w, dtrn, opt, predict; epochs=nepochs)\n",
    "else\n",
    "    for epoch = 1:nepochs\n",
    "        train(w, dtrn, opt, predict; epochs=1)\n",
    "        report(epoch, w, dtrn, dtst, predict)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! consider that we only used 1000 samples, and this time around we were able to achieve 93% accuracy on the `test` dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Contained in this example are nearly all the important ideas you'll need to start attacking problems in computer vision. While state-of-the-art vision systems incorporate a few more bells and whistles, they're all built on this foundation. Believe it or not, if you knew just the content in this tutorial 5 years ago, you could probably have sold a startup to a Fortune 500 company for millions of dollars. Fortunately (or unfortunately?), the world has gotten marginally more sophisticated, so we'll have to come up with some more sophisticated tutorials to follow."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.0",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
