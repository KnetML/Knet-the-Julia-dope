{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Julia Things"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment\n",
    "\n",
    "First things first. Let us set up the environment with the requried packages for this notebook. We will also set the desired context (e.g. `KnetArray` for gpu), the number of epochs (`nepochs`), and the variable `fast`. This variable is used to skip checking the accuracy at every epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OS: Linux\n",
      "Julia: 0.6.0\n",
      "Knet: 0.8.5+\n",
      "GPU: NVS 310\n",
      "TITAN X (Pascal)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for p in (\"Knet\", \"Plots\", \"Plotly.jl\")\n",
    "    Pkg.installed(p) == nothing && Pkg.add(p)\n",
    "end\n",
    "\n",
    "using Knet, Plots\n",
    "gr()\n",
    "\n",
    "Knet.gpu(0); # set the desired GPU to use\n",
    "atype   = KnetArray{Float32}; # atype = KnetArray{Float32} for gpu usage, Array{Float32} for cpu. \n",
    "nepochs = 10\n",
    "fast    = false\n",
    "println(\"OS: \", Sys.KERNEL)\n",
    "println(\"Julia: \", VERSION)\n",
    "println(\"Knet: \", Pkg.installed(\"Knet\"))\n",
    "println(\"GPU: \", readstring(`nvidia-smi --query-gpu=name --format=csv,noheader`))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Stuff\n",
    "\n",
    "In this notebook we introduce the following Julia/Knet packages and functions:\n",
    "\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer perceptrons from scratch\n",
    "\n",
    "In the previous chapters we showed how you could implement multiclass logistic regression \n",
    "(also called *softmax regression*)\n",
    "for classifiying images of handwritten digits into the 10 possible categories. \n",
    "This is where things start to get fun.\n",
    "We understand how to wrangle data, \n",
    "coerce our outputs into a valid probability distribution,\n",
    "how to apply an appropriate loss function,\n",
    "and how to optimize over our parameters.\n",
    "Now that we've covered these preliminaries, \n",
    "we can extend our toolbox to include deep neural networks.\n",
    "\n",
    "Recall that before, we mapped our inputs directly onto our outputs through a single linear transformation.\n",
    "$$\\hat{y} = \\mbox{softmax}(W \\boldsymbol{x} + b)$$\n",
    "\n",
    "Graphically, we could depict the model like this:\n",
    "![](https://github.com/zackchase/mxnet-the-straight-dope/blob/master/img/simple-softmax-net.png?raw=true)\n",
    "\n",
    "If our labels really were relatd to our input data by an approximately linear function,\n",
    "then this approah might be adequate.\n",
    "*But linearity is a strong assumption*.\n",
    "Linearity means that fixing one output of interest,\n",
    "for each input,\n",
    "increasing its value should either drive up the value of the output,\n",
    "or drive it down,\n",
    "irrespective of the value of the other inputs.\n",
    "\n",
    "Imagine the case of classifying cats and dogs based on black and white images.\n",
    "That's like saying that for each pixel, \n",
    "increasing its value either increases probability that it depicts a dog or decreases it.\n",
    "That's not reasonable. After all, the world contains both black dogs and black cats, and both white dogs and white cats. \n",
    "\n",
    "Teasing out what is depicted in an image generally requires allowing more complex relationships between\n",
    "our inputs and outputs, considering the possibility that our pattern might be characterized by interactions among the many features. \n",
    "In these cases, linear models will have low accuracy. \n",
    "We can model a more general class of functions by incorporating one or more *hidden layers*.\n",
    "The easiest way to do this is to stack a bunch of layers of neurons on top of each other.\n",
    "Each layer feeds in to the layer above it, until we generate an output.\n",
    "This architecture is commonly called a \"multilayer perceptron\".\n",
    "With an MLP, we're going to stack a bunch of layers on top of each other.\n",
    "\n",
    "$$ h_1 = \\phi(W_1\\boldsymbol{x} + b_1) $$\n",
    "$$ h_2 = \\phi(W_2\\boldsymbol{h_1} + b_2) $$\n",
    "$$...$$\n",
    "$$ h_n = \\phi(W_n\\boldsymbol{h_{n-1}} + b_n) $$\n",
    "\n",
    "Note that each layer requires its own set of parameters.\n",
    "For each hidden layer, we calculate its value by first applying a linear function \n",
    "to the acivatiosn of the layer below, and then applying an element-wise\n",
    "nonlinear activation function. \n",
    "Here, we've denoted the activation function for the hidden layers as $\\phi$.\n",
    "Finally, given the topmost hidden layer, we'll generate an output.\n",
    "Because we're still focusing on multiclass classification, we'll stick with the softmax activation in the output layer.\n",
    "\n",
    "$$ \\hat{y} = \\mbox{softmax}(W_y \\boldsymbol{h}_n + b_y)$$\n",
    "\n",
    "Graphically, a multilayer perceptron could be depicted like this:\n",
    "\n",
    "![](https://github.com/zackchase/mxnet-the-straight-dope/blob/master/img/multilayer-perceptron.png?raw=true)\n",
    "\n",
    "Multilayer perceptrons can account for complex interactions in the inputs because \n",
    "the hidden neurons depend on the values of each of the inputs. \n",
    "It's easy to design a hidden node that that do arbitrary computation,\n",
    "say logical operations.\n",
    "And it's even widely known that multilayer perceptrons are universal approximators. \n",
    "That means that even for a single-hidden-layer neural network,\n",
    "with enough nodes, and the right set of weights, it could model any function at all!\n",
    "Actually learning that function is the hard part. \n",
    "And it turns out that we can approximate functions much more compactly if we use deeper (vs wider) neural networks.\n",
    "We'll get more into the maths in subsequent chapter. But for now, let's actually build a MLP.\n",
    "In this example, we'll implement a multilayer perceptron with two hidden layers and one output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33mWARNING: \u001b[39m\u001b[22m\u001b[33mreplacing docs for 'mnisturl :: Union{}' in module 'Main'.\u001b[39m\n",
      "\u001b[1m\u001b[33mWARNING: \u001b[39m\u001b[22m\u001b[33mreplacing docs for 'mnistdir :: Union{}' in module 'Main'.\u001b[39m\n",
      "\u001b[1m\u001b[33mWARNING: \u001b[39m\u001b[22m\u001b[33mreplacing docs for 'mnist :: Tuple{}' in module 'Main'.\u001b[39m\n",
      "\u001b[1m\u001b[33mWARNING: \u001b[39m\u001b[22m\u001b[33mreplacing docs for 'mnistview :: Tuple{Any,Any}' in module 'Main'.\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "include(Knet.dir(\"data\",\"mnist.jl\"))\n",
    "xtrn, ytrn, xtst, ytst = mnist()\n",
    "dtrn = minibatch(xtrn, ytrn, 100, xtype=atype);\n",
    "dtst = minibatch(xtst, ytst, 100, xtype=atype);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "initweights (generic function with 2 methods)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function initweights(d, scale=0.01; hidden=[2], atype=Array{Float32})\n",
    "    model = Vector{Any}(2 * length(hidden))\n",
    "    X = d\n",
    "    for k = 1:length(hidden)\n",
    "        H = hidden[k]\n",
    "        model[2k - 1] = scale * randn(H, X) \n",
    "        model[2k]     = scale * randn(H, 1)\n",
    "        X = H\n",
    "    end\n",
    "    return map(atype, model)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define the function `initmodel` with the desired parameters. The variable `hidden` contains the output sizes for each of the layers, and `num_inputs` is the size of the input variable `x` (in this case $x\\in\\mathbb{R}^{784}$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "initmodel (generic function with 4 methods)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function initmodel(atype;num_inputs=784,num_hidden=256,num_outputs=10)\n",
    "    return initweights(num_inputs,hidden=[num_hidden,num_hidden,num_outputs]; atype=atype);\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict (generic function with 1 method)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function predict(w, x)\n",
    "    x = mat(x)\n",
    "    for i=1:2:length(w) - 2\n",
    "        x = relu.(w[i] * x .+ w[i+1])\n",
    "    end\n",
    "    return w[end - 1]*x .+ w[end]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the predict function to make sure everything works fine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×100 Knet.KnetArray{Float32,2}:\n",
       "  0.0227497     0.022383      0.0233163    …   0.023055      0.023645   \n",
       " -0.0011035    -0.00171302   -0.000232517     -0.0005282    -0.000880577\n",
       " -0.0114875    -0.0115701    -0.0104334       -0.00967458   -0.010482   \n",
       " -0.000505474   0.000431799  -8.67485f-5      -0.000775862  -0.00099238 \n",
       " -0.000354351   0.000189104  -0.00214156      -0.00062014   -0.00154479 \n",
       "  0.00512402    0.00518112    0.00480477   …   0.00500566    0.00480705 \n",
       " -0.0223992    -0.022344     -0.0207275       -0.0205362    -0.0206613  \n",
       " -0.00505681   -0.00434136   -0.00457164      -0.00460923   -0.00487216 \n",
       " -0.0160057    -0.0165691    -0.0150407       -0.016062     -0.0167797  \n",
       "  0.0114391     0.0104472     0.0122909        0.011399      0.0120771  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for (x, y) in dtrn\n",
    "    display(predict(initmodel(atype), x))\n",
    "    break\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss(w, x, ygold, predict) = nll(predict(w, x), ygold);\n",
    "lossgradient = grad(loss);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train (generic function with 2 methods)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train(w, dtrn, optim, predict; epochs=10)\n",
    "    for epoch = 1:epochs\n",
    "        for (x, y) in dtrn\n",
    "            g = lossgradient(w, x, y, predict)\n",
    "            update!(w, g, optim)\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optim(w; lr=0.01) = optimizers(w, Sgd;  lr=lr);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "report (generic function with 1 method)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function report(epoch, w, dtrn, dtst, predict)\n",
    "    println((:epoch, epoch, :trn, accuracy(w, dtrn, predict), :tst, accuracy(w, dtst, predict)))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(:epoch, 1, :trn, 0.8536166666666667, :tst, 0.856)\n",
      "(:epoch, 2, :trn, 0.9123333333333333, :tst, 0.9125)\n",
      "(:epoch, 3, :trn, 0.9393166666666667, :tst, 0.938)\n",
      "(:epoch, 4, :trn, 0.9544, :tst, 0.9521)\n",
      "(:epoch, 5, :trn, 0.9643833333333334, :tst, 0.9603)\n",
      "(:epoch, 6, :trn, 0.9713, :tst, 0.9667)\n",
      "(:epoch, 7, :trn, 0.97625, :tst, 0.9688)\n",
      "(:epoch, 8, :trn, 0.9797833333333333, :tst, 0.9713)\n",
      "(:epoch, 9, :trn, 0.9826, :tst, 0.9726)\n",
      "(:epoch, 10, :trn, 0.9842833333333333, :tst, 0.9735)\n"
     ]
    }
   ],
   "source": [
    "w   = initmodel(atype);\n",
    "opt = optim(w, lr=1e-1);\n",
    "\n",
    "if fast\n",
    "    train(w, dtrn, opt, predict; epochs=nepochs)\n",
    "else\n",
    "    for epoch = 1:nepochs\n",
    "        train(w, dtrn, opt, predict, epochs=1)\n",
    "        report(epoch, w, dtrn, dtst, predict)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Nice! With just two hidden layers containing 256 hidden nodes, respectively, we can achieve over 97% test accuracy on this task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next\n",
    "[dropout](section3-dropout.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For whinges or inquiries, [open an issue on  GitHub.](https://github.com/moralesq/Knet-the-Julia-dope)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.0",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
