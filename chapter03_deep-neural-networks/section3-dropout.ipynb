{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Julia Things"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment\n",
    "\n",
    "First things first. Let us set up the environment with the requried packages for this notebook. We will also set the desired context (e.g. `KnetArray` for gpu), the number of epochs (`nepochs`), and the variable `fast`. This variable is used to skip checking the accuracy at every epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OS: Linux\n",
      "Julia: 0.6.0\n",
      "Knet: 0.8.5+\n",
      "GPU: NVS 310\n",
      "TITAN X (Pascal)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for p in (\"Knet\", \"Plots\", \"Plotly.jl\")\n",
    "    Pkg.installed(p) == nothing && Pkg.add(p)\n",
    "end\n",
    "\n",
    "using Knet, Plots\n",
    "gr()\n",
    "\n",
    "Knet.gpu(0); # set the desired GPU to use\n",
    "atype   = KnetArray{Float32}; # atype = KnetArray{Float32} for gpu usage, Array{Float32} for cpu. \n",
    "nepochs = 10\n",
    "fast    = false\n",
    "pdrop   = 0.5\n",
    "println(\"OS: \", Sys.KERNEL)\n",
    "println(\"Julia: \", VERSION)\n",
    "println(\"Knet: \", Pkg.installed(\"Knet\"))\n",
    "println(\"GPU: \", readstring(`nvidia-smi --query-gpu=name --format=csv,noheader`))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Stuff\n",
    "\n",
    "In this notebook we introduce the following Julia/Knet packages and functions:\n",
    "\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout regularization \n",
    "\n",
    "If you're reading the tutorials in sequence, \n",
    "then you might remember from Part 2 \n",
    "that machine learning models \n",
    "can be susceptible to overfitting. \n",
    "To recap: in machine learning,\n",
    "our goal is to discover general patterns.\n",
    "For example, we might want to learn an association between genetic markers\n",
    "and the development of dementia in adulthood. \n",
    "Our hope would be to uncover a pattern that could be applied successfully to assess risk for the entire population.\n",
    "\n",
    "However, when we train models, we don't have access to the entire population (or current or potential humans).\n",
    "Instead, we can access only a small, finite sample.\n",
    "Even in a large hospital system, we might get hundreds of thousands of medical records. \n",
    "Given such a finite sample size, it's possible to uncover spurious associations \n",
    "that don't hold up for unseen data.\n",
    "\n",
    "Let's consider an extreme pathological case. \n",
    "Imagine that you want to learn to predict\n",
    "which people will repay their loans. \n",
    "A lender hires you as a data scientist \n",
    "to investigate the case and gives you complete files on 100 applicants,\n",
    "of which 5 defaulted on their loans within 3 years. \n",
    "The files might include hundreds of features \n",
    "including income, occupation, credit score, length of employment etcetera.\n",
    "Imagine that they additionally give you video footage of their interview with a lending agent. \n",
    "That might seem like a lot of data! \n",
    "\n",
    "Now suppose that after generating an enormous set of features,\n",
    "you discover that of the 5 applicants who defaults, \n",
    "all 5 were wearing blue shirts during their interviews,\n",
    "while only 40% of general population wore blue shirts. \n",
    "There's a good chance that any model you train would pick up on this signal \n",
    "and use it as an important part of its learned pattern.\n",
    "\n",
    "Even if defaulters are no more likely to wear blue shirts, \n",
    "there's a 1% chance that we'll observe all five defaulters wearing blue shirts.\n",
    "And keeping the sample size low while we have hundreds or thousands of features,\n",
    "we may observe a large number of spurious correlations. \n",
    "Given trillions of training examples, these false associations might disappear. \n",
    "But we seldom have that luxury.\n",
    "\n",
    "The phenomena of fitting our training distribution more closely than the real distribution\n",
    "is called *overfitting*, and the techniques used to combat overfitting are called regularization.\n",
    "In the previous chapter, we introduced one classical approach to regularize statistical models. \n",
    "We penalized the size (the $\\ell^2$ norm) of the weights, coercing them to take smaller values.\n",
    "In probabilistic terms we might say this imposes a Gaussian prior on the value of the weights. \n",
    "But in more intuitive, functional terms, we can say this encourages the model to spread out its weights among many features and not to depend too much on a small number of potentially spurious associations. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With great flexibility comes overfitting liability\n",
    "\n",
    "Given many more features than examples, linear models can overfit. \n",
    "But when there are many more examples than features, \n",
    "linear models can usually be counted on not to overfit.\n",
    "Unfortunately this propensity to generalize well comes at a cost. \n",
    "For every feature, a linear model has to assign it either positive or negative weight.\n",
    "Linear models can't take into account nuanced interactions between features.\n",
    "In more formal texts, you'll see this phenomena discussed as the bias-variance tradeoff.\n",
    "Linear models have high bias, (they can only represent a small class of functions),\n",
    "but low variance (they give similar results across different random samples of the data).\n",
    "[**point to more formal discussion of generalization when chapter exists**]\n",
    "\n",
    "Deep neural networks, however, occupy the opposite end of the bias-variance spectrum.\n",
    "Neural networks are so flexible because they aren't confined to looking at each feature individually.\n",
    "Instead, they can learn complex interactions among groups of features. \n",
    "For example, they might infer that \"Nigeria\" and \"Western Union\" \n",
    "appearing together in an email indicates spam \n",
    "but that \"Nigeria\" without \"Western Union\" does not connote spam. \n",
    "\n",
    "Even for a small number of features, deep neural networks are capable of overfitting.\n",
    "As one demonstration of the incredible flexibility of neural networks,\n",
    "researchers showed that [neural networks perfectly classify randomly labeled data](https://arxiv.org/abs/1611.03530).\n",
    "Let's think about what means. \n",
    "If the labels are assigned uniformly at random, and there are 10 classes, \n",
    "then no classifier can get better than 10% accuracy on holdout data. \n",
    "Yet even in these situations, when there is no true pattern to be learned, \n",
    "neural networks can perfectly fit the training labels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping out activations\n",
    "\n",
    "In 2012, Professor Geoffrey Hinton and his students including Nitish Srivastava \n",
    "introduced a new idea for how to regularize neural network models. \n",
    "The intuition goes something like this. \n",
    "When a neural network overfits badly to training data,\n",
    "each layer depends too heavily on the exact configuration\n",
    "of features in the previous layer. \n",
    "\n",
    "To prevent the neural network from depending too much on any exact activation pathway,\n",
    "Hinton and Srivastava proposed randomly *dropping out* (i.e. setting to $0$) \n",
    "the hidden nodes in every layer with probability $.5$.\n",
    "Given a network with $n$ nodes we are sampling uniformly at random from the $2^n$ \n",
    "networks in which a subset of the nodes are turned off. \n",
    "\n",
    "![](../img/dropout.png)\n",
    "\n",
    "One intuition here is that because the nodes to drop out are chosen randomly on every pass,\n",
    "the representations in each layer can't depend on the exact values taken by nodes in the previous layer. \n",
    "\n",
    "## Making predictions with dropout models\n",
    "\n",
    "However, when it comes time to make predictions, \n",
    "we want to use the full representational power of our model. \n",
    "In other words, we don't want to drop out activations at test time.\n",
    "One principled way to justify the use of all nodes simultaneously,\n",
    "despite not training in this fashion,\n",
    "is that it's a form of model averaging. \n",
    "At each layer we average the representations of all of the $2^n$ dropout networks.\n",
    "Because each node has a $.5$ probability of being on during training, \n",
    "its vote is scaled by $.5$ when we use all nodes at prediction time\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33mWARNING: \u001b[39m\u001b[22m\u001b[33mreplacing docs for 'cifarurl :: Union{}' in module 'Main'.\u001b[39m\n",
      "\u001b[1m\u001b[33mWARNING: \u001b[39m\u001b[22m\u001b[33mreplacing docs for 'cifardir :: Union{}' in module 'Main'.\u001b[39m\n",
      "\u001b[1m\u001b[33mWARNING: \u001b[39m\u001b[22m\u001b[33mreplacing docs for 'cifar10 :: Tuple{}' in module 'Main'.\u001b[39m\n",
      "\u001b[1m\u001b[33mWARNING: \u001b[39m\u001b[22m\u001b[33mreplacing docs for 'cifar100 :: Tuple{}' in module 'Main'.\u001b[39m\n",
      "\u001b[1m\u001b[33mWARNING: \u001b[39m\u001b[22m\u001b[33mreplacing docs for 'cifarview :: Tuple{Any,Any}' in module 'Main'.\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "include(Knet.dir(\"data\",\"cifar.jl\"))\n",
    "xtrn, ytrn, xtst, ytst = mnist()\n",
    "\n",
    "dtrn = minibatch(xtrn, ytrn, 100, xtype=atype);\n",
    "dtst = minibatch(xtst, ytst, 100, xtype=atype);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "initweights (generic function with 2 methods)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function initweights(d, scale=0.01; hidden=[2], atype=Array{Float32})\n",
    "    model = Vector{Any}(2 * length(hidden))\n",
    "    X = d\n",
    "    for k = 1:length(hidden)\n",
    "        H = hidden[k]\n",
    "        model[2k - 1] = scale * randn(H, X) \n",
    "        model[2k]     = scale * randn(H, 1)\n",
    "        X = H\n",
    "    end\n",
    "    return map(atype, model)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define the function `initmodel` with the desired parameters. The variable `hidden` contains the output sizes for each of the layers, and `num_inputs` is the size of the input variable `x` (in this case $x\\in\\mathbb{R}^{784}$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "initmodel(atype) = initweights(784, hidden=[256, 128, 10]; atype=atype);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function predict(w, x; p=0)\n",
    "    x = mat(x)\n",
    "    for i=1:2:length(w) - 2\n",
    "        x = relu.(w[i] * x .+ w[i+1])\n",
    "        x = dropout(x, p)\n",
    "    end\n",
    "    return w[end - 1]*x .+ w[end]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the predict function to make sure everything works fine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×100 Knet.KnetArray{Float32,2}:\n",
       "  0.0102376     0.0110553     0.00971472  …   0.0105167     0.0115427 \n",
       "  0.00719412    0.00809406    0.00717845      0.00660533    0.00637   \n",
       " -0.000235557   0.000569347   0.00010308      0.000691225  -0.00109118\n",
       " -0.0159943    -0.0154395    -0.0147362      -0.0158345    -0.0157569 \n",
       " -0.0134848    -0.0143681    -0.0139184      -0.0139141    -0.0141434 \n",
       "  0.00447695    0.00421254    0.00494863  …   0.0045765     0.00520394\n",
       " -0.00737216   -0.00750307   -0.00789365     -0.00717817   -0.00586199\n",
       "  0.00905698    0.00879994    0.00865319      0.0089237     0.00926897\n",
       " -0.00781964   -0.00838048   -0.00774621     -0.00739538   -0.00805289\n",
       "  0.00610901    0.00778764    0.00727687      0.00711902    0.00629103"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for (x, y) in dtrn\n",
    "    display(predict(initmodel(atype), x; p=pdrop))\n",
    "    break\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss(w, x, ygold, predict; o...) = nll(predict(w, x; o...), ygold);\n",
    "lossgradient = grad(loss);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.301888f0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for (x, y) in dtrn\n",
    "    display(loss(initmodel(atype), x, y, predict; p=pdrop))\n",
    "    break\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train(w, dtrn, optim, predict; epochs=10, o...)\n",
    "    for epoch = 1:epochs\n",
    "        for (x, y) in dtrn\n",
    "            g = lossgradient(w, x, y, predict; o...)\n",
    "            update!(w, g, optim)\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optim(w; lr=0.01) = optimizers(w, Sgd;  lr=lr);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "report (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function report(epoch, w, dtrn, dtst, predict)\n",
    "    println((:epoch, epoch, :trn, accuracy(w, dtrn, predict), :tst, accuracy(w, dtst, predict)))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(:epoch, 1, :trn, 0.84845, :tst, 0.8542)\n",
      "(:epoch, 2, :trn, 0.9113666666666667, :tst, 0.9118)\n",
      "(:epoch, 3, :trn, 0.9418, :tst, 0.94)\n",
      "(:epoch, 4, :trn, 0.9537333333333333, :tst, 0.9513)\n",
      "(:epoch, 5, :trn, 0.9601666666666666, :tst, 0.9574)\n",
      "(:epoch, 6, :trn, 0.9650666666666666, :tst, 0.9607)\n",
      "(:epoch, 7, :trn, 0.96965, :tst, 0.9651)\n",
      "(:epoch, 8, :trn, 0.9738666666666667, :tst, 0.9676)\n",
      "(:epoch, 9, :trn, 0.9758, :tst, 0.9697)\n",
      "(:epoch, 10, :trn, 0.9786166666666667, :tst, 0.9718)\n"
     ]
    }
   ],
   "source": [
    "w   = initmodel(atype);\n",
    "opt = optim(w, lr=1e-1);\n",
    "\n",
    "if fast\n",
    "    train(w, dtrn, opt, predict; epochs=nepochs, p=pdrop)\n",
    "else\n",
    "    for epoch = 1:nepochs\n",
    "        train(w, dtrn, opt, predict; epochs=1, p=pdrop)\n",
    "        report(epoch, w, dtrn, dtst, predict)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Nice. With just two hidden layers containing 256 and 128 hidden nodes, respectively, we can achieve over 95% accuracy on this task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next\n",
    "[dropout](section3-dropout.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For whinges or inquiries, [open an issue on  GitHub.](https://github.com/moralesq/Knet-the-Julia-dope)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.0",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
